{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75a321a3-c478-433d-9c25-fc4a00e69c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, uuid\n",
    "from typing import List, Dict, Tuple\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "load_dotenv()\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pypdf import PdfReader\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1ea27c7-d90b-490a-939d-c0dee0f4f92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyBU\n",
      "serp_api_key exists and begins c3ee9cec\n",
      "perplexity_api_key exists and begins pplx-r4f\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "serp_api_key = os.getenv(\"SERP_API_KEY\")\n",
    "perplexity_api_key = os.getenv(\"PEPLEXITY_API_KEY\")\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")\n",
    "\n",
    "if serp_api_key:\n",
    "    print(f\"serp_api_key exists and begins {serp_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"serp_api_key not set\")\n",
    "\n",
    "if perplexity_api_key:\n",
    "    print(f\"perplexity_api_key exists and begins {perplexity_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"perplexity_api_key not set\")\n",
    "\n",
    "# GPT ëª¨ë¸ ì„ ì–¸\n",
    "openai = OpenAI()\n",
    "MODEL = 'gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e781c6e1-ff16-4ec3-9d10-1639f2136eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_MODEL=\"gpt-4o-mini\"  # ì›í•˜ë©´ ë°”ê¾¸ì„¸ìš” (ex. gpt-4.1-mini)\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸ (ë¬¸ì„œ/ì§ˆì˜ ê³µí†µ, ë‹¤êµ­ì–´ ê°•ë ¥)\n",
    "EMBEDDING_MODEL=\"BAAI/bge-m3\"\n",
    "\n",
    "# ê²€ìƒ‰ ì„¤ì •\n",
    "TOP_K=6\n",
    "MMR=False             # Trueë¡œ ë°”ê¾¸ë©´ ë‹¤ì–‘ì„± ìš°ì„  ê²€ìƒ‰\n",
    "RERANK=False          # Trueë¡œ ë°”ê¾¸ë©´ cross-encoder ì¬ë­í‚¹(ì•„ë˜ ì˜µì…˜ ì°¸ê³ )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df1d8188-38d7-4250-96ac-d6290d8a8841",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"\n",
    "DB_DIR = \"vectordb\"\n",
    "COLLECTION_NAME = \"rag_collection\"\n",
    "\n",
    "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"BAAI/bge-m3\")\n",
    "CHUNK_TOKENS = 400\n",
    "CHUNK_OVERLAP = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04e447e3-e7e2-4905-9096-ce1477085ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INGEST] Embedding 1 chunks with BAAI/bge-m3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INGEST] Upserting to Chroma (rag_collection) ...\n",
      "[DONE] 1 files, 1 chunks indexed at vectordb/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------- ìœ í‹¸ ----------\n",
    "def read_text_file(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_pdf_file(path: str) -> str:\n",
    "    reader = PdfReader(path)\n",
    "    texts = []\n",
    "    for page in reader.pages:\n",
    "        txt = page.extract_text() or \"\"\n",
    "        texts.append(txt)\n",
    "    return \"\\n\".join(texts)\n",
    "\n",
    "def load_documents(data_dir: str) -> List[Tuple[str, str]]:\n",
    "    docs = []\n",
    "    patterns = [\"**/*.txt\", \"**/*.md\", \"**/*.pdf\"]\n",
    "    for pat in patterns:\n",
    "        for p in glob.glob(os.path.join(data_dir, pat), recursive=True):\n",
    "            ext = os.path.splitext(p)[1].lower()\n",
    "            if ext in [\".txt\", \".md\"]:\n",
    "                text = read_text_file(p)\n",
    "            elif ext == \".pdf\":\n",
    "                text = read_pdf_file(p)\n",
    "            else:\n",
    "                continue\n",
    "            if text.strip():\n",
    "                docs.append((p, text))\n",
    "    return docs\n",
    "\n",
    "def token_chunk(text: str, max_tokens=CHUNK_TOKENS, overlap=CHUNK_OVERLAP) -> List[str]:\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    toks = enc.encode(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(toks):\n",
    "        end = min(start + max_tokens, len(toks))\n",
    "        chunk = enc.decode(toks[start:end])\n",
    "        chunks.append(chunk)\n",
    "        if end == len(toks):\n",
    "            break\n",
    "        start = max(0, end - overlap)\n",
    "    return chunks\n",
    "\n",
    "# ---------- ì„ë² ë”© ----------\n",
    "def get_embedder():\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "    # bge/e5 ë¥˜ëŠ” ë³´í†µ normalize ì¶”ì²œ\n",
    "    model.max_seq_length = 512\n",
    "    return model\n",
    "\n",
    "# ---------- ë©”ì¸ ----------\n",
    "def main():\n",
    "    os.makedirs(DB_DIR, exist_ok=True)\n",
    "    client = chromadb.PersistentClient(path=DB_DIR, settings=Settings(allow_reset=False))\n",
    "    colls = [c.name for c in client.list_collections()]\n",
    "    if COLLECTION_NAME in colls:\n",
    "        collection = client.get_collection(COLLECTION_NAME)\n",
    "    else:\n",
    "        collection = client.create_collection(COLLECTION_NAME, metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "    embedder = get_embedder()\n",
    "\n",
    "    docs = load_documents(DATA_DIR)\n",
    "    if not docs:\n",
    "        print(f\"[INGEST] No documents found in ./{DATA_DIR}. Add files and rerun.\")\n",
    "        return\n",
    "\n",
    "    ids, texts, metadatas = [], [], []\n",
    "    for path, full_text in docs:\n",
    "        chunks = token_chunk(full_text)\n",
    "        for i, ch in enumerate(chunks):\n",
    "            ids.append(str(uuid.uuid4()))\n",
    "            texts.append(ch)\n",
    "            metadatas.append({\"source\": path, \"chunk_idx\": i})\n",
    "\n",
    "    print(f\"[INGEST] Embedding {len(texts)} chunks with {EMBEDDING_MODEL} ...\")\n",
    "    embs = embedder.encode(texts, normalize_embeddings=True, show_progress_bar=True).tolist()\n",
    "\n",
    "    print(f\"[INGEST] Upserting to Chroma ({COLLECTION_NAME}) ...\")\n",
    "    collection.upsert(ids=ids, embeddings=embs, metadatas=metadatas, documents=texts)\n",
    "\n",
    "    print(f\"[DONE] {len(docs)} files, {len(texts)} chunks indexed at {DB_DIR}/\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9792fd3-5442-40cd-a7bc-9dc5b739399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_answer(query: str, k: int = TOP_K, use_lexical: bool = True, use_rerank: bool = False) -> str:\n",
    "    hits = retrieve(query, k=k, mmr=USE_MMR)\n",
    "    if use_lexical:\n",
    "        hits = bm25_mix(query, hits)\n",
    "    if use_rerank or USE_RERANK:\n",
    "        hits = rerank_cross_encoder(query, hits)\n",
    "    answer = generate_answer(query, hits[:k])\n",
    "    sources = \"\\n\".join(\n",
    "        f\"[{i+1}] {h['meta'].get('source')} (chunk {h['meta'].get('chunk_idx')})\"\n",
    "        for i, h in enumerate(hits[:k], 1)\n",
    "    )\n",
    "    return f\"{answer}\\n\\n=== SOURCES ===\\n{sources}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36edb3b3-6bda-4bfc-b460-3c3a8504f21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collection():\n",
    "    client = chromadb.PersistentClient(path=DB_DIR, settings=Settings(allow_reset=False))\n",
    "    try:\n",
    "        return client.get_collection(COLLECTION_NAME)\n",
    "    except Exception:\n",
    "        raise RuntimeError(\n",
    "            f\"Chroma ì»¬ë ‰ì…˜ '{COLLECTION_NAME}'ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ingest.pyë¥¼ ì‹¤í–‰í•´ ìƒ‰ì¸í•˜ì„¸ìš”.\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58340698-44ac-477b-9c0d-b7eab47091bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Number of requested results 6 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ANSWER ===\n",
      "\n",
      "ì§ˆë¬¸ì´ êµ¬ì²´ì ìœ¼ë¡œ ë¬´ì—‡ì¸ì§€ ëª…ì‹œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì§ˆë¬¸ ë‚´ìš©ì„ ì œê³µí•´ ì£¼ì‹œë©´ ê·¸ì— ëŒ€í•œ ë‹µë³€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "=== SOURCES ===\n",
      "[1] data\\lg.txt (chunk 0)\n"
     ]
    }
   ],
   "source": [
    "import os, math\n",
    "from typing import List, Dict, Tuple\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "import argparse\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# ---------- ì„¤ì • ----------\n",
    "DB_DIR = \"vectordb\"\n",
    "COLLECTION_NAME = \"rag_collection\"\n",
    "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"BAAI/bge-m3\")\n",
    "TOP_K = int(os.getenv(\"TOP_K\", \"6\"))\n",
    "USE_MMR = os.getenv(\"MMR\", \"False\").lower() == \"true\"\n",
    "USE_RERANK = os.getenv(\"RERANK\", \"False\").lower() == \"true\"\n",
    "\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "\n",
    "# ---------- ì„ë² ë”© & DB ----------\n",
    "def get_embedder():\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "    model.max_seq_length = 512\n",
    "    return model\n",
    "\n",
    "def get_collection():\n",
    "    client = chromadb.PersistentClient(path=DB_DIR, settings=Settings(allow_reset=False))\n",
    "    return client.get_collection(COLLECTION_NAME)\n",
    "\n",
    "# ---------- ê²€ìƒ‰ ----------\n",
    "def retrieve(query: str, k: int = TOP_K, mmr: bool = USE_MMR) -> List[Dict]:\n",
    "    col = get_collection()\n",
    "    embedder = get_embedder()\n",
    "    q_emb = embedder.encode([query], normalize_embeddings=True).tolist()[0]\n",
    "\n",
    "    res = col.query(query_embeddings=[q_emb], n_results=max(k*3 if mmr else k, k))\n",
    "    docs = res[\"documents\"][0]\n",
    "    metas = res[\"metadatas\"][0]\n",
    "    dists = res[\"distances\"][0] if \"distances\" in res else [0.0]*len(docs)\n",
    "\n",
    "    items = [{\"doc\": d, \"meta\": m, \"dist\": dist} for d, m, dist in zip(docs, metas, dists)]\n",
    "\n",
    "    # ê°„ë‹¨ MMR(ìœ ì‚¬ë„ ë‹¤ì–‘ì„±) â€” ì„ë² ë”© ê¸°ë°˜ ì¬ì„ íƒ\n",
    "    if mmr:\n",
    "        emb_chunks = embedder.encode([it[\"doc\"] for it in items], normalize_embeddings=True)\n",
    "        selected = []\n",
    "        cand_idx = set(range(len(items)))\n",
    "        # greedy\n",
    "        while cand_idx and len(selected) < k:\n",
    "            if not selected:\n",
    "                # ê°€ì¥ ê°€ê¹Œìš´ ê²ƒë¶€í„°\n",
    "                best = min(cand_idx, key=lambda i: items[i][\"dist\"])\n",
    "                selected.append(best)\n",
    "                cand_idx.remove(best)\n",
    "                continue\n",
    "            # ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚°: í›„ë³´ì™€ ì´ë¯¸ ì„ íƒëœ ê²ƒë“¤ ê°„ ìµœëŒ€ ìœ ì‚¬ë„ ìµœì†Œí™”\n",
    "            def mmr_score(i):\n",
    "                import numpy as np\n",
    "                qsim = 1 - items[i][\"dist\"]  # cosine distance -> similarity approx\n",
    "                diversity = max(np.dot(emb_chunks[i], emb_chunks[j]) for j in selected)\n",
    "                lam = 0.75\n",
    "                return lam*qsim - (1-lam)*diversity\n",
    "            best = max(cand_idx, key=mmr_score)\n",
    "            selected.append(best)\n",
    "            cand_idx.remove(best)\n",
    "        items = [items[i] for i in selected]\n",
    "    else:\n",
    "        items = sorted(items, key=lambda x: x[\"dist\"])[:k]\n",
    "\n",
    "    return items\n",
    "\n",
    "# ---------- (ì˜µì…˜) Lexical + Re-Rank ----------\n",
    "def bm25_mix(query: str, hits: List[Dict], alpha: float = 0.2) -> List[Dict]:\n",
    "    \"\"\" ë²¡í„° íˆíŠ¸ì—ì„œ BM25 ì ìˆ˜ì™€ í˜¼í•© (alphaëŠ” BM25 ê°€ì¤‘ì¹˜) \"\"\"\n",
    "    corpus = [h[\"doc\"] for h in hits]\n",
    "    bm25 = BM25Okapi([c.split() for c in corpus])\n",
    "    bm_scores = bm25.get_scores(query.split())\n",
    "    # ê±°ë¦¬(dist)ëŠ” ë‚®ì„ìˆ˜ë¡ ìœ ë¦¬ â†’ ìœ ì‚¬ë„ì²˜ëŸ¼ ë°”ê¿”ì„œ í•©ì‚°\n",
    "    import numpy as np\n",
    "    dist = np.array([h[\"dist\"] for h in hits])\n",
    "    sim = 1 - (dist / (dist.max() + 1e-9))\n",
    "    mixed = alpha*bm_scores + (1-alpha)*sim\n",
    "    order = mixed.argsort()[::-1]\n",
    "    return [hits[i] for i in order]\n",
    "\n",
    "def rerank_cross_encoder(query: str, hits: List[Dict], model_name: str = \"BAAI/bge-reranker-v2-m3\") -> List[Dict]:\n",
    "    ce = CrossEncoder(model_name)\n",
    "    pairs = [[query, h[\"doc\"]] for h in hits]\n",
    "    scores = ce.predict(pairs)\n",
    "    ranked = sorted(zip(hits, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [h for h, _ in ranked]\n",
    "\n",
    "# ---------- ìƒì„± ----------\n",
    "def build_prompt(query: str, contexts: List[Dict]) -> List[Dict]:\n",
    "    context_block = \"\\n\\n\".join(\n",
    "        [f\"[{i+1}] SOURCE: {c['meta'].get('source')} (chunk {c['meta'].get('chunk_idx')})\\n{c['doc']}\"\n",
    "         for i, c in enumerate(contexts)]\n",
    "    )\n",
    "    system = (\n",
    "        \"ë‹¹ì‹ ì€ ì •í™•í•œ RAG ë¹„ì„œì…ë‹ˆë‹¤. ì œê³µëœ 'ì»¨í…ìŠ¤íŠ¸'ë§Œ ê·¼ê±°ë¡œ í•œêµ­ì–´ë¡œ ë‹µí•˜ì„¸ìš”. \"\n",
    "        \"ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”. ë°˜ë“œì‹œ ê·¼ê±°ê°€ ëœ ì¶œì²˜ë¥¼ ì¸ë±ìŠ¤ ë²ˆí˜¸ë¡œ í•¨ê»˜ í‘œê¸°í•˜ì„¸ìš”.\"\n",
    "    )\n",
    "    user = (\n",
    "        f\"ì§ˆë¬¸:\\n{query}\\n\\n\"\n",
    "        f\"ì»¨í…ìŠ¤íŠ¸(ì°¸ì¡°ìš©):\\n{context_block}\\n\\n\"\n",
    "        \"ìš”êµ¬ì‚¬í•­:\\n- ì»¨í…ìŠ¤íŠ¸ ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ ì¶”ì¸¡ ê¸ˆì§€\\n- í•µì‹¬ ìš”ì•½ â†’ ê·¼ê±° í‘œê¸° [1], [2]...\\n\"\n",
    "    )\n",
    "    return [\n",
    "        {\"role\":\"system\", \"content\": system},\n",
    "        {\"role\":\"user\", \"content\": user}\n",
    "    ]\n",
    "\n",
    "def generate_answer(query: str, contexts: List[Dict]) -> str:\n",
    "    client = OpenAI()\n",
    "    messages = build_prompt(query, contexts)\n",
    "    resp = client.chat.completions.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        messages=messages,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "# ---------- CLI ----------\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--q\", required=True, help=\"ì§ˆë¬¸ í…ìŠ¤íŠ¸\")\n",
    "    parser.add_argument(\"--k\", type=int, default=TOP_K, help=\"ê²€ìƒ‰ ê°œìˆ˜\")\n",
    "    parser.add_argument(\"--lexical\", action=\"store_true\", help=\"BM25 ê°€ì¤‘ í˜¼í•© ì‚¬ìš©\")\n",
    "    parser.add_argument(\"--rerank\", action=\"store_true\", help=\"Cross-Encoder ì¬ë­í‚¹ ì‚¬ìš©\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    hits = retrieve(args.q, k=args.k, mmr=USE_MMR)\n",
    "    if args.lexical:\n",
    "        hits = bm25_mix(args.q, hits)\n",
    "    if args.rerank or USE_RERANK:\n",
    "        hits = rerank_cross_encoder(args.q, hits)\n",
    "\n",
    "    answer = generate_answer(args.q, hits[:args.k])\n",
    "\n",
    "    print(\"\\n=== ANSWER ===\\n\")\n",
    "    print(answer.strip())\n",
    "    print(\"\\n=== SOURCES ===\")\n",
    "    for i, h in enumerate(hits[:args.k], 1):\n",
    "        src = h[\"meta\"].get(\"source\"); idx = h[\"meta\"].get(\"chunk_idx\")\n",
    "        print(f\"[{i}] {src} (chunk {idx})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        IN_JUPYTER = get_ipython() is not None\n",
    "    except Exception:\n",
    "        IN_JUPYTER = False\n",
    "\n",
    "    if IN_JUPYTER and len(sys.argv) == 1:\n",
    "        print(\"ğŸ” ì£¼í”¼í„° ê°ì§€: ì•„ë˜ ì¤‘ í•˜ë‚˜ë¡œ ì‹¤í–‰í•˜ì„¸ìš”.\\n\"\n",
    "              \"1) %run rag.py --q 'ì§ˆë¬¸' --k 6 --lexical\\n\"\n",
    "              \"2) !python rag.py --q 'ì§ˆë¬¸' --k 6 --lexical\\n\"\n",
    "              \"3) (ë…¸íŠ¸ë¶ ì…€) sys.argv ì§€ì • í›„ main() í˜¸ì¶œ\")\n",
    "    else:\n",
    "        main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0c22c6f-9821-443c-9418-6ac0d7517352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rag from: C:\\Users\\HyunJunLee\\Documents\\hj_git\\llm_proposal_mvp\\news_project\\RAG_test\\rag.py\n",
      "rag.rag_answer is ready!\n"
     ]
    }
   ],
   "source": [
    "import rag\n",
    "from importlib import reload\n",
    "reload(rag)  # rag.pyë¥¼ ë°©ê¸ˆ ìˆ˜ì •í–ˆë‹¤ë©´ ë‹¤ì‹œ ë¡œë“œ\n",
    "\n",
    "# ì–´ë–¤ rag.pyê°€ ì„í¬íŠ¸ëëŠ”ì§€ í™•ì¸(ê²½ë¡œ í™•ì¸ìš©)\n",
    "print(\"Loaded rag from:\", rag.__file__)\n",
    "\n",
    "def _rag_answer(query: str, k: int = 6, use_lexical: bool = True, use_rerank: bool = False) -> str:\n",
    "    hits = rag.retrieve(query, k=k, mmr=rag.USE_MMR)\n",
    "    if use_lexical:\n",
    "        hits = rag.bm25_mix(query, hits)\n",
    "    if use_rerank or rag.USE_RERANK:\n",
    "        hits = rag.rerank_cross_encoder(query, hits)\n",
    "    answer = rag.generate_answer(query, hits[:k])\n",
    "    src_lines = []\n",
    "    for i, h in enumerate(hits[:k], 1):\n",
    "        src = h[\"meta\"].get(\"source\"); idx = h[\"meta\"].get(\"chunk_idx\")\n",
    "        src_lines.append(f\"[{i}] {src} (chunk {idx})\")\n",
    "    return f\"{answer}\\n\\n=== SOURCES ===\\n\" + \"\\n\".join(src_lines)\n",
    "\n",
    "# ëª¨ë“ˆì— í•¨ìˆ˜ ë¶€ì°©\n",
    "rag.rag_answer = _rag_answer\n",
    "print(\"rag.rag_answer is ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbd465f3-f2ba-4d90-8565-8ceb0a71b1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Q&A ëª¨ë“œì…ë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'quit' ì…ë ¥.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Q>  lgëŠ” ì–´ë–¤ ì‹œìŠ¤í…œì„ì¨?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ANSWER ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 6 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGëŠ” AEM, GP1, Magento, Middleware, Jira ë“±ì˜ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•©ë‹ˆë‹¤. [1]\n",
      "\n",
      "=== SOURCES ===\n",
      "[1] data\\lg.txt (chunk 0)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Q>  weekly ë¯¸íŒ…ì€ ì–´ë–»ê²Œ ì§„í–‰ë˜ì§€?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ANSWER ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 6 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weekly ë¯¸íŒ…ì€ Jira ë‚´ì—ì„œ closedëœ í‹°ì¼“ ìˆ˜, WIP(ì§„í–‰ ì¤‘ì¸ ì‘ì—…) í‹°ì¼“ ìˆ˜, ë¦¬ë“œíƒ€ì„(í‹°ì¼“ ì™„ë£Œë˜ê¸°ê¹Œì§€ ê±¸ë¦° ì‹œê°„) ë“±ì˜ ì£¼ê°„ í˜„í™©ì„ í™•ì¸í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì§„í–‰ëœë‹¤. [1]\n",
      "\n",
      "=== SOURCES ===\n",
      "[1] data\\lg.txt (chunk 0)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Q>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¢…ë£Œí•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import rag  # rag.pyê°€ ê°™ì€ í´ë”ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "def chat(k=6, use_lexical=True, use_rerank=False):\n",
    "    print(\"RAG Q&A ëª¨ë“œì…ë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'quit' ì…ë ¥.\")\n",
    "    while True:\n",
    "        try:\n",
    "            q = input(\"\\nQ> \").strip()\n",
    "        except (EOFError, KeyboardInterrupt):\n",
    "            print(\"\\nì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "            break\n",
    "\n",
    "        if q.lower() in (\"exit\", \"quit\"):\n",
    "            print(\"ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "            break\n",
    "        if not q:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            print(\"\\n--- ANSWER ---\")\n",
    "            ans = rag.rag_answer(q, k=k, use_lexical=use_lexical, use_rerank=use_rerank)\n",
    "            print(ans)\n",
    "        except Exception as e:\n",
    "            print(f\"[ì—ëŸ¬] {type(e).__name__}: {e}\")\n",
    "            print(\"Â· ingest.py ë¨¼ì € ì‹¤í–‰í–ˆëŠ”ì§€, Â· OPENAI_API_KEY, Â· ë„¤íŠ¸ì›Œí¬, Â· rag.py ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "chat(k=6, use_lexical=True, use_rerank=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
