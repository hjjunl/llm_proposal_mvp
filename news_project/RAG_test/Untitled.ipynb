{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75a321a3-c478-433d-9c25-fc4a00e69c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, uuid\n",
    "from typing import List, Dict, Tuple\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "load_dotenv()\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pypdf import PdfReader\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1ea27c7-d90b-490a-939d-c0dee0f4f92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyBU\n",
      "serp_api_key exists and begins c3ee9cec\n",
      "perplexity_api_key exists and begins pplx-r4f\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "serp_api_key = os.getenv(\"SERP_API_KEY\")\n",
    "perplexity_api_key = os.getenv(\"PEPLEXITY_API_KEY\")\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")\n",
    "\n",
    "if serp_api_key:\n",
    "    print(f\"serp_api_key exists and begins {serp_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"serp_api_key not set\")\n",
    "\n",
    "if perplexity_api_key:\n",
    "    print(f\"perplexity_api_key exists and begins {perplexity_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"perplexity_api_key not set\")\n",
    "\n",
    "# GPT 모델 선언\n",
    "openai = OpenAI()\n",
    "MODEL = 'gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e781c6e1-ff16-4ec3-9d10-1639f2136eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_MODEL=\"gpt-4o-mini\"  # 원하면 바꾸세요 (ex. gpt-4.1-mini)\n",
    "\n",
    "# 임베딩 모델 (문서/질의 공통, 다국어 강력)\n",
    "EMBEDDING_MODEL=\"BAAI/bge-m3\"\n",
    "\n",
    "# 검색 설정\n",
    "TOP_K=6\n",
    "MMR=False             # True로 바꾸면 다양성 우선 검색\n",
    "RERANK=False          # True로 바꾸면 cross-encoder 재랭킹(아래 옵션 참고)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df1d8188-38d7-4250-96ac-d6290d8a8841",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"\n",
    "DB_DIR = \"vectordb\"\n",
    "COLLECTION_NAME = \"rag_collection\"\n",
    "\n",
    "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"BAAI/bge-m3\")\n",
    "CHUNK_TOKENS = 400\n",
    "CHUNK_OVERLAP = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04e447e3-e7e2-4905-9096-ce1477085ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INGEST] Embedding 1 chunks with BAAI/bge-m3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|███████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INGEST] Upserting to Chroma (rag_collection) ...\n",
      "[DONE] 1 files, 1 chunks indexed at vectordb/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------- 유틸 ----------\n",
    "def read_text_file(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_pdf_file(path: str) -> str:\n",
    "    reader = PdfReader(path)\n",
    "    texts = []\n",
    "    for page in reader.pages:\n",
    "        txt = page.extract_text() or \"\"\n",
    "        texts.append(txt)\n",
    "    return \"\\n\".join(texts)\n",
    "\n",
    "def load_documents(data_dir: str) -> List[Tuple[str, str]]:\n",
    "    docs = []\n",
    "    patterns = [\"**/*.txt\", \"**/*.md\", \"**/*.pdf\"]\n",
    "    for pat in patterns:\n",
    "        for p in glob.glob(os.path.join(data_dir, pat), recursive=True):\n",
    "            ext = os.path.splitext(p)[1].lower()\n",
    "            if ext in [\".txt\", \".md\"]:\n",
    "                text = read_text_file(p)\n",
    "            elif ext == \".pdf\":\n",
    "                text = read_pdf_file(p)\n",
    "            else:\n",
    "                continue\n",
    "            if text.strip():\n",
    "                docs.append((p, text))\n",
    "    return docs\n",
    "\n",
    "def token_chunk(text: str, max_tokens=CHUNK_TOKENS, overlap=CHUNK_OVERLAP) -> List[str]:\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    toks = enc.encode(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(toks):\n",
    "        end = min(start + max_tokens, len(toks))\n",
    "        chunk = enc.decode(toks[start:end])\n",
    "        chunks.append(chunk)\n",
    "        if end == len(toks):\n",
    "            break\n",
    "        start = max(0, end - overlap)\n",
    "    return chunks\n",
    "\n",
    "# ---------- 임베딩 ----------\n",
    "def get_embedder():\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "    # bge/e5 류는 보통 normalize 추천\n",
    "    model.max_seq_length = 512\n",
    "    return model\n",
    "\n",
    "# ---------- 메인 ----------\n",
    "def main():\n",
    "    os.makedirs(DB_DIR, exist_ok=True)\n",
    "    client = chromadb.PersistentClient(path=DB_DIR, settings=Settings(allow_reset=False))\n",
    "    colls = [c.name for c in client.list_collections()]\n",
    "    if COLLECTION_NAME in colls:\n",
    "        collection = client.get_collection(COLLECTION_NAME)\n",
    "    else:\n",
    "        collection = client.create_collection(COLLECTION_NAME, metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "    embedder = get_embedder()\n",
    "\n",
    "    docs = load_documents(DATA_DIR)\n",
    "    if not docs:\n",
    "        print(f\"[INGEST] No documents found in ./{DATA_DIR}. Add files and rerun.\")\n",
    "        return\n",
    "\n",
    "    ids, texts, metadatas = [], [], []\n",
    "    for path, full_text in docs:\n",
    "        chunks = token_chunk(full_text)\n",
    "        for i, ch in enumerate(chunks):\n",
    "            ids.append(str(uuid.uuid4()))\n",
    "            texts.append(ch)\n",
    "            metadatas.append({\"source\": path, \"chunk_idx\": i})\n",
    "\n",
    "    print(f\"[INGEST] Embedding {len(texts)} chunks with {EMBEDDING_MODEL} ...\")\n",
    "    embs = embedder.encode(texts, normalize_embeddings=True, show_progress_bar=True).tolist()\n",
    "\n",
    "    print(f\"[INGEST] Upserting to Chroma ({COLLECTION_NAME}) ...\")\n",
    "    collection.upsert(ids=ids, embeddings=embs, metadatas=metadatas, documents=texts)\n",
    "\n",
    "    print(f\"[DONE] {len(docs)} files, {len(texts)} chunks indexed at {DB_DIR}/\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9792fd3-5442-40cd-a7bc-9dc5b739399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_answer(query: str, k: int = TOP_K, use_lexical: bool = True, use_rerank: bool = False) -> str:\n",
    "    hits = retrieve(query, k=k, mmr=USE_MMR)\n",
    "    if use_lexical:\n",
    "        hits = bm25_mix(query, hits)\n",
    "    if use_rerank or USE_RERANK:\n",
    "        hits = rerank_cross_encoder(query, hits)\n",
    "    answer = generate_answer(query, hits[:k])\n",
    "    sources = \"\\n\".join(\n",
    "        f\"[{i+1}] {h['meta'].get('source')} (chunk {h['meta'].get('chunk_idx')})\"\n",
    "        for i, h in enumerate(hits[:k], 1)\n",
    "    )\n",
    "    return f\"{answer}\\n\\n=== SOURCES ===\\n{sources}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36edb3b3-6bda-4bfc-b460-3c3a8504f21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collection():\n",
    "    client = chromadb.PersistentClient(path=DB_DIR, settings=Settings(allow_reset=False))\n",
    "    try:\n",
    "        return client.get_collection(COLLECTION_NAME)\n",
    "    except Exception:\n",
    "        raise RuntimeError(\n",
    "            f\"Chroma 컬렉션 '{COLLECTION_NAME}'이 없습니다. 먼저 ingest.py를 실행해 색인하세요.\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58340698-44ac-477b-9c0d-b7eab47091bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Number of requested results 6 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ANSWER ===\n",
      "\n",
      "질문이 구체적으로 무엇인지 명시되지 않았습니다. 질문 내용을 제공해 주시면 그에 대한 답변을 드리겠습니다.\n",
      "\n",
      "=== SOURCES ===\n",
      "[1] data\\lg.txt (chunk 0)\n"
     ]
    }
   ],
   "source": [
    "import os, math\n",
    "from typing import List, Dict, Tuple\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "import argparse\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# ---------- 설정 ----------\n",
    "DB_DIR = \"vectordb\"\n",
    "COLLECTION_NAME = \"rag_collection\"\n",
    "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"BAAI/bge-m3\")\n",
    "TOP_K = int(os.getenv(\"TOP_K\", \"6\"))\n",
    "USE_MMR = os.getenv(\"MMR\", \"False\").lower() == \"true\"\n",
    "USE_RERANK = os.getenv(\"RERANK\", \"False\").lower() == \"true\"\n",
    "\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "\n",
    "# ---------- 임베딩 & DB ----------\n",
    "def get_embedder():\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "    model.max_seq_length = 512\n",
    "    return model\n",
    "\n",
    "def get_collection():\n",
    "    client = chromadb.PersistentClient(path=DB_DIR, settings=Settings(allow_reset=False))\n",
    "    return client.get_collection(COLLECTION_NAME)\n",
    "\n",
    "# ---------- 검색 ----------\n",
    "def retrieve(query: str, k: int = TOP_K, mmr: bool = USE_MMR) -> List[Dict]:\n",
    "    col = get_collection()\n",
    "    embedder = get_embedder()\n",
    "    q_emb = embedder.encode([query], normalize_embeddings=True).tolist()[0]\n",
    "\n",
    "    res = col.query(query_embeddings=[q_emb], n_results=max(k*3 if mmr else k, k))\n",
    "    docs = res[\"documents\"][0]\n",
    "    metas = res[\"metadatas\"][0]\n",
    "    dists = res[\"distances\"][0] if \"distances\" in res else [0.0]*len(docs)\n",
    "\n",
    "    items = [{\"doc\": d, \"meta\": m, \"dist\": dist} for d, m, dist in zip(docs, metas, dists)]\n",
    "\n",
    "    # 간단 MMR(유사도 다양성) — 임베딩 기반 재선택\n",
    "    if mmr:\n",
    "        emb_chunks = embedder.encode([it[\"doc\"] for it in items], normalize_embeddings=True)\n",
    "        selected = []\n",
    "        cand_idx = set(range(len(items)))\n",
    "        # greedy\n",
    "        while cand_idx and len(selected) < k:\n",
    "            if not selected:\n",
    "                # 가장 가까운 것부터\n",
    "                best = min(cand_idx, key=lambda i: items[i][\"dist\"])\n",
    "                selected.append(best)\n",
    "                cand_idx.remove(best)\n",
    "                continue\n",
    "            # 다양성 점수 계산: 후보와 이미 선택된 것들 간 최대 유사도 최소화\n",
    "            def mmr_score(i):\n",
    "                import numpy as np\n",
    "                qsim = 1 - items[i][\"dist\"]  # cosine distance -> similarity approx\n",
    "                diversity = max(np.dot(emb_chunks[i], emb_chunks[j]) for j in selected)\n",
    "                lam = 0.75\n",
    "                return lam*qsim - (1-lam)*diversity\n",
    "            best = max(cand_idx, key=mmr_score)\n",
    "            selected.append(best)\n",
    "            cand_idx.remove(best)\n",
    "        items = [items[i] for i in selected]\n",
    "    else:\n",
    "        items = sorted(items, key=lambda x: x[\"dist\"])[:k]\n",
    "\n",
    "    return items\n",
    "\n",
    "# ---------- (옵션) Lexical + Re-Rank ----------\n",
    "def bm25_mix(query: str, hits: List[Dict], alpha: float = 0.2) -> List[Dict]:\n",
    "    \"\"\" 벡터 히트에서 BM25 점수와 혼합 (alpha는 BM25 가중치) \"\"\"\n",
    "    corpus = [h[\"doc\"] for h in hits]\n",
    "    bm25 = BM25Okapi([c.split() for c in corpus])\n",
    "    bm_scores = bm25.get_scores(query.split())\n",
    "    # 거리(dist)는 낮을수록 유리 → 유사도처럼 바꿔서 합산\n",
    "    import numpy as np\n",
    "    dist = np.array([h[\"dist\"] for h in hits])\n",
    "    sim = 1 - (dist / (dist.max() + 1e-9))\n",
    "    mixed = alpha*bm_scores + (1-alpha)*sim\n",
    "    order = mixed.argsort()[::-1]\n",
    "    return [hits[i] for i in order]\n",
    "\n",
    "def rerank_cross_encoder(query: str, hits: List[Dict], model_name: str = \"BAAI/bge-reranker-v2-m3\") -> List[Dict]:\n",
    "    ce = CrossEncoder(model_name)\n",
    "    pairs = [[query, h[\"doc\"]] for h in hits]\n",
    "    scores = ce.predict(pairs)\n",
    "    ranked = sorted(zip(hits, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [h for h, _ in ranked]\n",
    "\n",
    "# ---------- 생성 ----------\n",
    "def build_prompt(query: str, contexts: List[Dict]) -> List[Dict]:\n",
    "    context_block = \"\\n\\n\".join(\n",
    "        [f\"[{i+1}] SOURCE: {c['meta'].get('source')} (chunk {c['meta'].get('chunk_idx')})\\n{c['doc']}\"\n",
    "         for i, c in enumerate(contexts)]\n",
    "    )\n",
    "    system = (\n",
    "        \"당신은 정확한 RAG 비서입니다. 제공된 '컨텍스트'만 근거로 한국어로 답하세요. \"\n",
    "        \"모르면 모른다고 말하세요. 반드시 근거가 된 출처를 인덱스 번호로 함께 표기하세요.\"\n",
    "    )\n",
    "    user = (\n",
    "        f\"질문:\\n{query}\\n\\n\"\n",
    "        f\"컨텍스트(참조용):\\n{context_block}\\n\\n\"\n",
    "        \"요구사항:\\n- 컨텍스트 범위를 벗어난 추측 금지\\n- 핵심 요약 → 근거 표기 [1], [2]...\\n\"\n",
    "    )\n",
    "    return [\n",
    "        {\"role\":\"system\", \"content\": system},\n",
    "        {\"role\":\"user\", \"content\": user}\n",
    "    ]\n",
    "\n",
    "def generate_answer(query: str, contexts: List[Dict]) -> str:\n",
    "    client = OpenAI()\n",
    "    messages = build_prompt(query, contexts)\n",
    "    resp = client.chat.completions.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        messages=messages,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "# ---------- CLI ----------\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--q\", required=True, help=\"질문 텍스트\")\n",
    "    parser.add_argument(\"--k\", type=int, default=TOP_K, help=\"검색 개수\")\n",
    "    parser.add_argument(\"--lexical\", action=\"store_true\", help=\"BM25 가중 혼합 사용\")\n",
    "    parser.add_argument(\"--rerank\", action=\"store_true\", help=\"Cross-Encoder 재랭킹 사용\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    hits = retrieve(args.q, k=args.k, mmr=USE_MMR)\n",
    "    if args.lexical:\n",
    "        hits = bm25_mix(args.q, hits)\n",
    "    if args.rerank or USE_RERANK:\n",
    "        hits = rerank_cross_encoder(args.q, hits)\n",
    "\n",
    "    answer = generate_answer(args.q, hits[:args.k])\n",
    "\n",
    "    print(\"\\n=== ANSWER ===\\n\")\n",
    "    print(answer.strip())\n",
    "    print(\"\\n=== SOURCES ===\")\n",
    "    for i, h in enumerate(hits[:args.k], 1):\n",
    "        src = h[\"meta\"].get(\"source\"); idx = h[\"meta\"].get(\"chunk_idx\")\n",
    "        print(f\"[{i}] {src} (chunk {idx})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        IN_JUPYTER = get_ipython() is not None\n",
    "    except Exception:\n",
    "        IN_JUPYTER = False\n",
    "\n",
    "    if IN_JUPYTER and len(sys.argv) == 1:\n",
    "        print(\"🔎 주피터 감지: 아래 중 하나로 실행하세요.\\n\"\n",
    "              \"1) %run rag.py --q '질문' --k 6 --lexical\\n\"\n",
    "              \"2) !python rag.py --q '질문' --k 6 --lexical\\n\"\n",
    "              \"3) (노트북 셀) sys.argv 지정 후 main() 호출\")\n",
    "    else:\n",
    "        main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0c22c6f-9821-443c-9418-6ac0d7517352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rag from: C:\\Users\\HyunJunLee\\Documents\\hj_git\\llm_proposal_mvp\\news_project\\RAG_test\\rag.py\n",
      "rag.rag_answer is ready!\n"
     ]
    }
   ],
   "source": [
    "import rag\n",
    "from importlib import reload\n",
    "reload(rag)  # rag.py를 방금 수정했다면 다시 로드\n",
    "\n",
    "# 어떤 rag.py가 임포트됐는지 확인(경로 확인용)\n",
    "print(\"Loaded rag from:\", rag.__file__)\n",
    "\n",
    "def _rag_answer(query: str, k: int = 6, use_lexical: bool = True, use_rerank: bool = False) -> str:\n",
    "    hits = rag.retrieve(query, k=k, mmr=rag.USE_MMR)\n",
    "    if use_lexical:\n",
    "        hits = rag.bm25_mix(query, hits)\n",
    "    if use_rerank or rag.USE_RERANK:\n",
    "        hits = rag.rerank_cross_encoder(query, hits)\n",
    "    answer = rag.generate_answer(query, hits[:k])\n",
    "    src_lines = []\n",
    "    for i, h in enumerate(hits[:k], 1):\n",
    "        src = h[\"meta\"].get(\"source\"); idx = h[\"meta\"].get(\"chunk_idx\")\n",
    "        src_lines.append(f\"[{i}] {src} (chunk {idx})\")\n",
    "    return f\"{answer}\\n\\n=== SOURCES ===\\n\" + \"\\n\".join(src_lines)\n",
    "\n",
    "# 모듈에 함수 부착\n",
    "rag.rag_answer = _rag_answer\n",
    "print(\"rag.rag_answer is ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbd465f3-f2ba-4d90-8565-8ceb0a71b1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Q&A 모드입니다. 종료하려면 'exit' 또는 'quit' 입력.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Q>  lg는 어떤 시스템을써?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ANSWER ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 6 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LG는 AEM, GP1, Magento, Middleware, Jira 등의 시스템을 사용합니다. [1]\n",
      "\n",
      "=== SOURCES ===\n",
      "[1] data\\lg.txt (chunk 0)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Q>  weekly 미팅은 어떻게 진행되지?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ANSWER ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 6 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weekly 미팅은 Jira 내에서 closed된 티켓 수, WIP(진행 중인 작업) 티켓 수, 리드타임(티켓 완료되기까지 걸린 시간) 등의 주간 현황을 확인하는 방식으로 진행된다. [1]\n",
      "\n",
      "=== SOURCES ===\n",
      "[1] data\\lg.txt (chunk 0)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Q>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "종료합니다.\n"
     ]
    }
   ],
   "source": [
    "import rag  # rag.py가 같은 폴더에 있어야 합니다.\n",
    "\n",
    "def chat(k=6, use_lexical=True, use_rerank=False):\n",
    "    print(\"RAG Q&A 모드입니다. 종료하려면 'exit' 또는 'quit' 입력.\")\n",
    "    while True:\n",
    "        try:\n",
    "            q = input(\"\\nQ> \").strip()\n",
    "        except (EOFError, KeyboardInterrupt):\n",
    "            print(\"\\n종료합니다.\")\n",
    "            break\n",
    "\n",
    "        if q.lower() in (\"exit\", \"quit\"):\n",
    "            print(\"종료합니다.\")\n",
    "            break\n",
    "        if not q:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            print(\"\\n--- ANSWER ---\")\n",
    "            ans = rag.rag_answer(q, k=k, use_lexical=use_lexical, use_rerank=use_rerank)\n",
    "            print(ans)\n",
    "        except Exception as e:\n",
    "            print(f\"[에러] {type(e).__name__}: {e}\")\n",
    "            print(\"· ingest.py 먼저 실행했는지, · OPENAI_API_KEY, · 네트워크, · rag.py 경로를 확인하세요.\")\n",
    "\n",
    "chat(k=6, use_lexical=True, use_rerank=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
